import pandas as pd
from sklearn.ensemble import RandomForestClassifier

class MLStrategy(Strategy):
    window_size = 500
    retrain_every = 30
    
    def init(self):
        # Initialize model and features
        self.model = RandomForestClassifier()
        self.features = []
        self.targets = []
        
    def next(self):
        # 1. Store latest data
        latest = pd.DataFrame({
            'close': self.data.Close[-1],
            'volume': self.data.Volume[-1],
            'rsi': self.I(RSI, self.data.Close, 14)[-1]
        }, index=[0])
        
        # 2. Retrain periodically
        if len(self.data) % self.retrain_every == 0 and len(self.data) > self.window_size:
            X = pd.DataFrame(self.features[-self.window_size:])
            y = self.targets[-self.window_size:]
            self.model.fit(X, y)
            
        # 3. Generate signal
        if len(self.data) > 50:  # Wait for enough data
            prediction = self.model.predict(latest[['close', 'volume', 'rsi']])
            if prediction == 1:
                self.buy()
            else:
                self.sell()
        
        # 4. Store for next retrain (using future return as target)
        if len(self.data) > 5:
            future_return = (self.data.Close[-5] / self.data.Close[-1] - 1) > 0
            self.features.append(latest.values[0])
            self.targets.append(future_return)

import pandas as pd
import numpy as np

def add_lagged_features(df, feature_cols, lag_days=7):
    """
    Adds lagged features to a DataFrame with datetime index.
    
    Args:
        df: DataFrame with datetime index and features (e.g., ['Close', 'Volume'])
        feature_cols: List of columns to create lags for (e.g., ['Close', 'Volume'])
        lag_days: Number of lag days to create (e.g., 7 for 7-day lags)
    
    Returns:
        DataFrame with original data + lagged features
    """
    df_lagged = df.copy()
    
    # For each feature (Close, Volume, etc.)
    for col in feature_cols:
        # For each lag period (1d, 2d, ..., 7d)
        for lag in range(1, lag_days + 1):
            new_col_name = f"{col}_lag_{lag}d"
            df_lagged[new_col_name] = df[col].shift(lag)
    
    # Remove rows with NaN (due to lagging)
    df_lagged = df_lagged.dropna()
    
    return df_lagged

import pandas as pd
from sklearn.ensemble import RandomForestClassifier

class MLStrategy(Strategy):
    window_size = 500
    retrain_every = 30
    
    def init(self):
        # Initialize model and features
        self.model = RandomForestClassifier()
        self.features = []
        self.targets = []
        
    def next(self):
        # 1. Store latest data
        latest = pd.DataFrame({
            'close': self.data.Close[-1],
            'volume': self.data.Volume[-1],
            'rsi': self.I(RSI, self.data.Close, 14)[-1]
        }, index=[0])
        
        # 2. Retrain periodically
        if len(self.data) % self.retrain_every == 0 and len(self.data) > self.window_size:
            X = pd.DataFrame(self.features[-self.window_size:])
            y = self.targets[-self.window_size:]
            self.model.fit(X, y)
            
        # 3. Generate signal
        if len(self.data) > 50:  # Wait for enough data
            prediction = self.model.predict(latest[['close', 'volume', 'rsi']])
            if prediction == 1:
                self.buy()
            else:
                self.sell()
        
        # 4. Store for next retrain (using future return as target)
        if len(self.data) > 5:
            future_return = (self.data.Close[-5] / self.data.Close[-1] - 1) > 0
            self.features.append(latest.values[0])
            self.targets.append(future_return)

from sklearn.feature_selection import SelectFromModel
from sklearn.linear_model import Lasso, LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
import numpy as np
import pandas as pd

def select_important_features(X, y, n_features=10, task_type='regression', 
                            regularization='l1', random_state=42):
    """
    Selects top N features using regularized models.
    
    Parameters:
    -----------
    X : pd.DataFrame or np.array
        Feature matrix (samples x features)
    y : pd.Series or np.array
        Target variable
    n_features : int
        Number of top features to select
    task_type : str ('regression' or 'classification')
        Type of machine learning task
    regularization : str ('l1' or 'l2')
        Type of regularization penalty
    random_state : int
        Random seed for reproducibility
        
    Returns:
    --------
    tuple: (selected_feature_names, importance_scores)
    """
    # Input validation
    if isinstance(X, pd.DataFrame):
        feature_names = X.columns.tolist()
    else:
        feature_names = [f'feature_{i}' for i in range(X.shape[1])]
    
    X = np.array(X)
    y = np.array(y)
    
    # Choose model based on task
    if task_type == 'regression':
        model = Lasso(alpha=0.01, random_state=random_state)
    elif task_type == 'classification':
        model = LogisticRegression(penalty=regularization, C=0.1, 
                                 solver='liblinear', 
                                 random_state=random_state)
    else:
        raise ValueError("task_type must be 'regression' or 'classification'")
    
    # Create pipeline
    pipe = Pipeline([
        ('scaler', StandardScaler()),
        ('selector', SelectFromModel(model, max_features=n_features))
    ])
    
    # Fit selector
    pipe.fit(X, y)
    
    # Get selected features
    selected_mask = pipe.named_steps['selector'].get_support()
    selected_features = [feature_names[i] for i in range(len(feature_names)) 
                        if selected_mask[i]]
    
    # Get importance scores
    if task_type == 'regression':
        importance = np.abs(pipe.named_steps['selector'].estimator_.coef_)
    else:
        importance = np.abs(pipe.named_steps['selector'].estimator_.coef_[0])
    
    importance_scores = {feature_names[i]: importance[i] 
                        for i in range(len(feature_names))}
    
    return selected_features, importance_scores

# Example Usage:
if __name__ == "__main__":
    from sklearn.datasets import make_classification
    
    # Create synthetic data
    X, y = make_classification(n_samples=1000, n_features=20, 
                              n_informative=5, random_state=42)
    X = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(20)])
    
    # Get top 5 features
    selected, scores = select_important_features(
        X, y, 
        n_features=5, 
        task_type='classification',
        regularization='l1'
    )
    
    print("Top Features:", selected)
    print("Importance Scores:", {k: round(v, 4) for k, v in sorted(scores.items(), 
                                              key=lambda x: -abs(x[1]))[:5]})
def execute_order_66(self, row_of_data: Union[np.array, pd.DataFrame]):
    """
    Processes one new data point and generates trading signals.
    Implements the full workflow:
    1. Update rolling window
    2. Feature engineering
    3. Model prediction
    4. Signal generation
    
    Returns:
        str: "buy", "sell", or "hold"
    """
    # --- 1. Data Preparation ---
    # Convert input to numpy if needed
    if isinstance(row_of_data, pd.DataFrame):
        row_values = row_of_data.values.reshape(1, -1)
    else:
        row_values = np.array(row_of_data).reshape(1, -1)
    
    # Initialize data window if empty
    if not hasattr(self, 'data_window'):
        self.data_window = np.zeros((self.window_model, row_values.shape[1]))
    
    # Update rolling window (FIFO)
    self.data_window = np.vstack([self.data_window[1:], row_values])
    
    # Check if we have enough data
    if len(self.data_window) < self.window_model:
        return "hold"
    
    # --- 2. Feature Engineering ---
    # Power transform
    scaled_data = self.transform_data(self.data_window)
    
    # Add polynomial features
    poly_data = self._add_slop(slop_degree=2, latest_data=scaled_data)
    
    # Add lag features
    feature_cols = list(range(poly_data.shape[1]))  # All columns
    lagged_data = self._add_lags(
        pd.DataFrame(poly_data), 
        feature_cols=feature_cols,
        lag_days=7
    )
    
    # --- 3. Model Predictions ---
    # Feature selection
    X_filtered = self._select_important_features(
        adjusted_data=lagged_data,
        returns_=self.data_window[:, 0],  # Assuming col 0 is returns
        n_features=10,
        regulization='l1'
    )
    
    # Price prediction
    price_pred = self._predict_model_price(
        latest_data=X_filtered[-1:],  # Most recent features
        model=self.price_model
    )
    
    # Classification (buy/hold/sell)
    action = self._predict_model_triplet(
        latest_data=X_filtered[-1:],
        model=self.classifier_model
    )
    
    # --- 4. Risk Management ---
    # Example: Don't buy if predicted return < threshold
    if action == "buy" and price_pred < 0.005:  # 0.5% minimum
        return "hold"
    
    # Example: Force close if extreme loss predicted
    if action == "hold" and price_pred < -0.03:  # -3%
        return "sell"
    
    return action